# -*- coding: utf-8 -*-
"""
Created on Thu Feb 22 21:20:54 2024

@author: Vivek
"""


"""
A pharmaceuticals manufacturing company is conducting a study on a new medicine
to treat heart diseases. The company has gathered data from its secondary 
sources and would like you to provide high level analytical insights on the data.
Its aim is to segregate patients depending on their age group and other factors 
given in the data. Perform PCA and clustering algorithms on the dataset and 
check if the clusters formed before and after PCA are the same and provide 
a brief report on your model.You can also explore more ways to improve your model. 

Business Problem- 
Q.What is the business objective?
Cardiovascular disease is the most common cause of mortality in developed countries. 
Across the globe, the incidence of death from cardiovascular and circulatory 
diseases has risen by one third between 1990 and 2010,such that by 2015 one in 
three deaths worldwide will be due to cardiovascular diseases.Epidemiologic studies 
have played an important role in elucidating the factors that predispose to 
cardiovascular disease and highlighting opportunities for prevention

Q.Are there any constraints?
It is difficult to manually determine the odds of getting heart disease based on 
risk factors. However,  machine learning techniques are usefulto predict the 
output from existing data.


"""


#Let us apply K-means algorithm

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.pylab as plt
from sklearn.cluster import KMeans

pharma=pd.read_csv("C:\Data Set\heart_disease.csv")
#EDA
pharma.dtypes

pharma.describe()
#show the distribution of wine contents.There is average 13 alcohal 2.33 malic and 746.89 is Proline
# avaerage age of patient is 54,min is 29 and max is 77

plt.hist(data = pharma, x = 'age');
#This is apparently a normal distribution.Most of heart diseases occured in group 50 to 60

plt.hist(data = pharma, x = 'cp');
# chest pain of type 0 is higher than other three types
plt.hist(data = pharma, x = 'sex');
# Male=1,and male are having more heart disease tendency than females
plt.hist(data = pharma, x = 'trestbps');
#This is normal distribution and resting blood pressure is higher in range 120 to 140
plt.hist(data = pharma, x = 'chol');
#This is normal distribution
plt.hist(data = pharma, x = 'fbs');
#(fasting blood sugar &gt; 120 mg/dl) (1 = true; 0 = false)
plt.hist(data = pharma, x = 'restecg');
#resting electrocardiographic results
plt.hist(data = pharma, x = 'thalach');
#maximum heart rate achieved is normal distribution

plt.hist(data = pharma, x = 'exang');
#exercise induced angina (1 = yes; 0 = no)
pharma.age.value_counts()
#2    71
#1    59
#3    48

###Data preprocessing
pharma.isna().sum()

# we know that there is scale difference among the columns,which we have to remove
#either by using normalization or standardization
def norm_func(i):
    x=(i-i.min())/(i.max()-i.min())
    return x


# Now apply this normalization function to airlines datframe for all the rows and column from 1 until end

df_norm=norm_func(pharma.iloc[:,:])
TWSS=[]
k=list(range(2,14))
# The values generated by TWSS are 12 and two get x and y values 12 by 12 ,range has been changed 2:14

for i in k:
    kmeans=KMeans(n_clusters=i)
    kmeans.fit(df_norm)
    TWSS.append(kmeans.inertia_)
TWSS

plt.plot(k,TWSS,'ro-');plt.xlabel("No_of_clusters");plt.ylabel("Total_within_SS")
# from the plot it is clear that the TWSS is reducing from k=2 to 3 and 3 to 4 
#than any other change in values of k,hence k=3 is selected
model=KMeans(n_clusters=3)
model.fit(df_norm)
model.labels_
mb=pd.Series(model.labels_)
pharma['clust']=mb
#wine.head()
pharma=pharma.iloc[:,[14,0,1,2,3,4,5,6,7,8,9,10,11,12,13]]
pharma.clust.value_counts()
#0    132
#2    95
#3    76


from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn.preprocessing import scale 

# Considering only numerical data 
pharma.data = pharma.iloc[:,1:]

# Normalizing the numerical data 
pharma_normal = scale(pharma.data)
pharma_normal

pca = PCA(n_components = 6)
pca_values = pca.fit_transform(pharma_normal)

# The amount of variance that each PCA explains is 
var = pca.explained_variance_ratio_
var

# PCA weights
pca.components_
pca.components_[0]

# Cumulative variance 
var1 = np.cumsum(np.round(var, decimals = 4) * 100)
var1

# Variance plot for PCA components obtained 
plt.plot(var1, color = "red")

# PCA scores
pca_values

pca_data = pd.DataFrame(pca_values)
pca_data.columns = "comp0", "comp1", "comp2", "comp3", "comp4", "comp5"
final = pd.concat([pharma.clust, pca_data.iloc[:, 0:3]], axis = 1)

final1=final.iloc[:,1:]
# Now let us apply K-means to PCA converted final1
TWSS=[]
k=list(range(2,14))
# The values generated by TWSS are 12 and two get x and y values 12 by 12 ,range has been changed 2:14


for i in k:
    kmeans=KMeans(n_clusters=i)
    kmeans.fit(final1)
    TWSS.append(kmeans.inertia_)
TWSS

plt.plot(k,TWSS,'ro-');plt.xlabel("No_of_clusters");plt.ylabel("Total_within_SS")
# from the plot it is clear that the TWSS is reducing from k=2 to 3 and 3 to 4 
#than any other change in values of k,hence k=3 is selected
model=KMeans(n_clusters=3)
model.fit(final1)
model.labels_
mb=pd.Series(model.labels_)
final['clust1']=mb

final.clust.value_counts()
#0    132
#2     95
#1     76
final.clust1.value_counts()
#0    113     #85 % correct clustering
#2    115    #82 % correct clustering
#1     75     #98 % correct clustering


#Aglomerative clustering
import pandas as pd
import matplotlib.pylab as plt
# Now import file from data set and create a dataframe
pharma2=pd.read_csv("C:\Data Set\heart_disease.csv")

# we know that there is scale difference among the columns,which we have to remove
#either by using normalization or standardization
def norm_func(i):
    x=(i-i.min())/(i.max()-i.min())
    return x
# Now apply this normalization function to crime datframe for all the rows and column from 1 until end
    
df_norm=norm_func(pharma2.iloc[:,:])
# you can check the df_norm dataframe which is scaled between values from 0 to1
# you can apply describe function to new data frame
df_norm.describe()

# Now to create dendrogram, we need to measure distance,we have to import linkage
from scipy.cluster.hierarchy import linkage
import scipy.cluster.hierarchy as sch
z=linkage(df_norm,method="complete",metric="euclidean")
plt.figure(figsize=(15,8));plt.title("Hierarchical Clustering dendrogram");plt.xlabel("Index");plt.ylabel("Distance")
sch.dendrogram(z,leaf_rotation=0,leaf_font_size=10)
plt.show()

# applying agglomerative clustering choosing 11 as clusters from dendrogram
from sklearn.cluster import AgglomerativeClustering
h_complete=AgglomerativeClustering(n_clusters=3,linkage='complete',affinity="euclidean").fit(df_norm)
# apply labels to the clusters
h_complete.labels_
cluster_labels=pd.Series(h_complete.labels_)
#Assign this series to Univ Dataframe as column and name the column as "clust"
pharma2['clust']=cluster_labels
# we want to relocate the column 66 to 0 th position
pharma2=pharma2.iloc[:,[14,0,1,2,3,4,5,6,7,8,9,10,11,12,13]]
# let us check the value count in wine3 Type columns
pharma2.clust.value_counts()
#0    133
#1    101
#2     69
# After application of PCA to pharma
final.clust1.value_counts()
#0    113     #85 % correct clustering
#2    115    #88 % correct clustering
#1     75     #92 % correct clustering

